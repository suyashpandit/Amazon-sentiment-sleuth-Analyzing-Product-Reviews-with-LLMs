{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31793905-ab79-40fb-8869-9eb325c8ab3d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightseagreen; color: black; padding: 4px; text-align : center\">\n",
    "    <h3>Model Code\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b36f65-5680-414f-8985-da20c721953c",
   "metadata": {},
   "source": [
    "**This notebook contains only the model code with no explanations or markdowns. The last cell can be run to print the classification report to see the model performance if needed.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2609be09-da22-4fce-8049-044fc2e9339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\suyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import numpy as np #for numerical computing\n",
    "import pandas as pd #for data handling and manipulation\n",
    "import matplotlib.pyplot as plt #for data visualization\n",
    "import seaborn as sns #for data visualization\n",
    "import nltk #natural language tool kit\n",
    "from nltk.corpus import stopwords #stopwords\n",
    "from nltk.tokenize import word_tokenize #tokenizer\n",
    "import string #convert to lowercase\n",
    "from nltk.stem import WordNetLemmatizer #lemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag #to assign POS tags to words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #to convert text to matrix of features\n",
    "from sklearn.model_selection import train_test_split #to split the data into training and testing sets for model evaluation \n",
    "from sklearn.linear_model import LogisticRegression #to built the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay #model evaluation and visualization\n",
    "nltk.download(\"punkt_tab\") #for tokenization\n",
    "nltk.download(\"stopwords\") #download the stop words list\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\") #for pos tagging\n",
    "nltk.download(\"wordnet\") #for lexical database\n",
    "nltk.download(\"omw-1.4\") #extended support for lemmatization\n",
    "\n",
    "#Dataset loading\n",
    "df = pd.read_csv('amazon_alexa.tsv', sep='\\t')\n",
    "\n",
    "#Lowercasing\n",
    "df[\"lowercase_reviews\"] = [str(review).lower() for review in df[\"verified_reviews\"]]\n",
    "\n",
    "#Tokenization\n",
    "df[\"tokens\"] = [word_tokenize(review) for review in df[\"lowercase_reviews\"]]\n",
    "df[\"tokens_no_punct\"] = [[word for word in tokens if word not in string.punctuation] for tokens in df[\"tokens\"]]\n",
    "\n",
    "#Stop word removal\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "df[\"no_stopwords_token\"] = [[word for word in tokens if word not in stop_words] for tokens in df[\"tokens_no_punct\"]]\n",
    "\n",
    "#Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict ={\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "df[\"final_lemmatized_clean_tokens\"] = [[lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens] for tokens in df[\"no_stopwords_token\"]]\n",
    "\n",
    "#Sentiment Labeling\n",
    "df[\"sentiment\"] =[\"Positive\" if rating >=3 else \"Negative\" for rating in df[\"rating\"]]\n",
    "\n",
    "#Creating a string from tokens\n",
    "df[\"final_clean_text\"] = [\" \".join(tokens) for tokens in df [\"final_lemmatized_clean_tokens\"]]\n",
    "\n",
    "#Converting text to numerical values \n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df[\"final_clean_text\"])\n",
    "\n",
    "#Assigning y to the sentiment column\n",
    "y = df[\"sentiment\"]#assigning y to the sentiment column\n",
    "\n",
    "#Splitting dataset to train and test set, with test set being 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 35)\n",
    "\n",
    "#Logistic Regression model\n",
    "model = LogisticRegression(class_weight= {'Negative': 10, 'Positive': 1})\n",
    "model.fit(X_train, y_train)#fitting the model\n",
    "\n",
    "#Predicting on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82db4d53-7b41-4820-96b7-f16e3154591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.43      0.80      0.56        50\n",
      "    Positive       0.98      0.91      0.94       580\n",
      "\n",
      "    accuracy                           0.90       630\n",
      "   macro avg       0.71      0.86      0.75       630\n",
      "weighted avg       0.94      0.90      0.91       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))#printing evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86dcf5a-268a-421b-8372-299af57e83db",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightseagreen; color: black; padding: 4px; text-align: center\">\n",
    "    <h3>Thank you!\n",
    "</h3> </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
